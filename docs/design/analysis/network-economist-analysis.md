# Network Economist Analysis: Capability Taxonomy Architecture

**Perspective**: Network Economist
**Focus**: Incentives, governance, standards adoption, network effects
**Date**: 2026-02-12

---

## Executive Summary

The capability taxonomy is a coordination mechanism that will determine whether SEP achieves network effects necessary for viability. Historical evidence shows: **taxonomies succeed when they reduce transaction costs more than they impose adoption costs, and fail when governance friction exceeds matching benefit**.

The central insight from WIR, Sardex, ITEX, LETS, and TimeBanks: **successful vocabularies are those where the cost of learning and using is substantially lower than the cost of matching without it**.

**Recommendation**: Hybrid architecture with curated core for Phase 1, AI-mediated semantic bridging for flexibility, graduated governance model.

---

## The Economics of Classification

### Why Taxonomies Exist

| Function | Without Taxonomy | With Taxonomy |
|----------|------------------|---------------|
| Search costs | High | Low |
| Verification costs | High | Lower |
| Matching precision | Low | Higher |
| Learning curve | Per-counterparty | Once |
| Network effects | Weak | Strong |

### Historical Systems Analysis

**SIC Codes**: Authoritative governance creates adoption but also bottlenecks. 10+ year revision cycles would be fatal for SEP.

**NAICS**: Even with regular revision, centralised governance can't keep pace with economic evolution.

**O*NET Extension Model**: Core taxonomy (SOC) provides structure; enrichment layer (skills) provides depth. SEP could follow this pattern.

---

## Incentive Structures

### Costs of Taxonomy Participation

| Cost | Source | Mitigation |
|------|--------|------------|
| Learning curve | Understanding categories | Guided onboarding, AI |
| Classification effort | Choosing categories | AI suggestion + confirmation |
| Maintenance burden | Updating when offerings change | Automated prompts |
| Governance participation | Reviewing/proposing | Delegate to interested parties |

### Incentive Design

1. **Match quality signalling**: Better categorisation â†’ better matches
2. **Contribution recognition**: Taxonomy contributor badges
3. **Graduated complexity**: AI handles taxonomy for new users
4. **Outcome-linked learning**: Successful exchanges reinforce patterns

---

## Governance Models

### Model Comparison

| Model | Advantages | Disadvantages | Burden |
|-------|------------|---------------|--------|
| Centralised Curation | Consistency, quality control | Bottleneck, capture risk | HIGH |
| Federated Extensions | Local adaptation, distributed | Fragmentation, translation | MEDIUM |
| Market Governance | No bottleneck, adaptive | Quality control, gaming | LOW |
| AI-Mediated | Pattern detection, speed | Opacity, trust required | MEDIUM |

### Recommended Evolution

| Phase | Primary Model | Rationale |
|-------|---------------|-----------|
| Phase 1 | Centralised Curation | Small network, need consistency |
| Phase 2 | Centralised Core + Federated Extensions | Scaling beyond central capacity |
| Phase 3 | AI-Mediated + Human Oversight | Sufficient data for AI learning |

---

## Network Effects and Critical Mass

### Category Viability Thresholds

- Minimum providers per category: ~5
- Minimum recipients with needs: ~3
- Minimum participants per category: ~8

### Network Viability Thresholds

| Participants | Network Effects |
|--------------|-----------------|
| Below 100 | Taxonomy feels sparse; manual curation essential |
| 100-500 | Effects emerging; AI can begin learning |
| 500-2000 | Strong effects; taxonomy self-sustaining |
| 2000+ | Full effects; competitive advantage |

### Building to Critical Mass

**Pre-launch**: Design for target sectors, recruit anchors, ensure no empty categories
**Launch**: Focus on well-populated categories, hide sparse ones
**Growth**: Expand based on demand, consolidate underused categories

---

## Standards Adoption Dynamics

### Success Factors

1. **Clear value proposition**: Immediate matching benefit
2. **Low adoption cost**: AI handles complexity
3. **Authoritative governance**: Credible stewardship
4. **Interoperability**: Map to O*NET, Schema.org, NAICS
5. **Appropriate granularity**: Match user mental models

### Failure Factors

1. **Governance dysfunction**: Slow updates, capture
2. **Insufficient coverage**: Gaps for emerging capabilities
3. **Excessive complexity**: Specialist training required
4. **Competing standards**: Lost to alternatives

---

## Commons Management (Ostrom Principles)

| Principle | SEP Application |
|-----------|-----------------|
| Clear boundaries | Define who can propose/vote on changes |
| Congruence | Rules match participatory ethos |
| Collective choice | Participants have voice |
| Monitoring | Track taxonomy health metrics |
| Graduated sanctions | Escalating response to mis-categorisation |
| Conflict resolution | Clear process for disputes |
| Nested enterprises | Network-level extensions within global framework |

---

## Tragedy of the Commons Risks

### Risk: Quality Degradation (Over-Specificity)

**Scenario**: Everyone adds narrow categories, fragmenting matching pools
**Mitigation**: Usage thresholds, consolidation process, AI conflation

### Risk: Gaming (Mis-Categorisation)

**Scenario**: Providers claim many categories to appear in more searches
**Mitigation**: Limited selections per offering, recipient feedback, reputation impact

### Risk: Governance Capture

**Scenario**: Large participants push definitions favouring their offerings
**Mitigation**: Transparent process, distributed voting, appeal process

### Risk: Maintenance Abandonment

**Scenario**: No one proposes additions; governance committee inactive
**Mitigation**: Funded governance function, AI gap detection, scheduled reviews

---

## Architecture Economic Analysis

| Option | Transaction Costs | Network Effects | Governance Cost |
|--------|------------------|-----------------|-----------------|
| Canonical + Curated | Low for participants | Strong | HIGH |
| Canonical + Extensions | Medium | Medium-Strong | Distributed |
| Semantic Layer | Very Low | Variable | None |
| Emergent | Unpredictable | Uncertain | None |
| **Hybrid** | Low | Strong | Medium |

---

## Recommendations

### Architecture

**Phase 1**: Canonical core (50-100 categories) + AI semantic bridging
- Core for structure and AI training
- AI bridges natural language
- Human confirms AI categorisation
- Central governance for core

**Phase 2**: Core + Network extensions + AI bridging
- Networks add extensions for local needs
- AI bridges across extensions
- AI proposes additions based on usage

**Phase 3**: AI-mediated governance + Market signals
- AI proposes changes
- Usage patterns validate
- Human oversight for significant changes

### Risk Mitigation

| Risk | Mitigation |
|------|------------|
| Commons degradation | Usage thresholds, consolidation |
| Gaming | Limited selections, feedback |
| Capture | Transparent process, distributed voting |
| Abandonment | Funded governance, AI detection |

### Key Metrics

- Category coverage: % of offerings fitting existing categories
- Match rate: % of needs finding matches via taxonomy
- Categorisation accuracy: AI suggestions confirmed by users
- Governance backlog: Time from proposal to decision
